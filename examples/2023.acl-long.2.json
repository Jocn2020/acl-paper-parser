{"publication_detail": {"title": "SAFECONV: Explaining and Correcting Conversational Unsafe Behavior", "authors": ["Zhang, Mian", "Lifeng, Jin", "Song, Linfeng", "Haitao, \u22c4", "Chen, Wenliang", "Yu, Dong"], "pub_date": ""}, "abstract": "\nOne of the main challenges open-domain endto-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SAFECONV for the research of conversational safety: (1) Besides the utterancelevel safety labels, SAFECONV also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SAFECONV provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory.By virtue of the comprehensive annotation of SAFECONV, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. The dataset is available at https://github.com/mianzhang/SafeConv.\n", "citations": [{"citation_text": "Safety of artificial intelligence models is a topic that attracts mounting attention and concerns from the community (Challen et al., 2019). In this work, we focus on the safety of open-domain conversational models, or chatbots.", "publication": {"title": "Artificial intelligence, bias and clinical safety", "authors": ["Challen, Robert", "Denny, Joshua", "Pitt, Martin", "Gompels, Luke", "Edwards, Tom", "Tsaneva-Atanasova, Krasimira"], "pub_date": "2019"}}, {"citation_text": "On the right, two methods generating an alternative response are compared. 2017) trained end-to-end with Language Modeling objectives on large corpora (Radford et al., 2019;Zhang et al., 2020;Wang et al., 2020), where offensive, unreliable and toxic content may exist (Gehman et al., 2020). Thus there are risks for these chatbots to generate responses with unsafe behavior, such as direct offensiveness, agreement to a toxic statement or harmful advice, reflecting patterns learned from the training data (Wolf et al., 2017;Nozza et al., 2021).", "publication": {"title": "Language models are unsupervised multitask learners", "authors": ["Radford, Alec", "Wu, Jeffrey", "Child, Rewon", "Luan, David", "Amodei, Dario", "Sutskever, Ilya"], "pub_date": "2019"}}, {"citation_text": "On the right, two methods generating an alternative response are compared. 2017) trained end-to-end with Language Modeling objectives on large corpora (Radford et al., 2019;Zhang et al., 2020;Wang et al., 2020), where offensive, unreliable and toxic content may exist (Gehman et al., 2020). Thus there are risks for these chatbots to generate responses with unsafe behavior, such as direct offensiveness, agreement to a toxic statement or harmful advice, reflecting patterns learned from the training data (Wolf et al., 2017;Nozza et al., 2021).", "publication": {"title": "DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation", "authors": ["Zhang, Yizhe", "Sun, Siqi", "Galley, Michel", "Chen, Yen-Chun", "Brockett, Chris", "Gao, Xiang", "Gao, Jianfeng", "Liu, Jingjing", "Dolan, Bill"], "pub_date": "2020"}}, {"citation_text": "On the right, two methods generating an alternative response are compared. 2017) trained end-to-end with Language Modeling objectives on large corpora (Radford et al., 2019;Zhang et al., 2020;Wang et al., 2020), where offensive, unreliable and toxic content may exist (Gehman et al., 2020). Thus there are risks for these chatbots to generate responses with unsafe behavior, such as direct offensiveness, agreement to a toxic statement or harmful advice, reflecting patterns learned from the training data (Wolf et al., 2017;Nozza et al., 2021).", "publication": {"title": "A Large-Scale Chinese Short-Text Conversation Dataset", "authors": ["Wang, Yida", "Ke, Pei", "Zheng, Yinhe", "Huang, Kaili", "Jiang, Yong", "Zhu, Xiaoyan", "Huang, Minlie"], "pub_date": "2020"}}, {"citation_text": "On the right, two methods generating an alternative response are compared. 2017) trained end-to-end with Language Modeling objectives on large corpora (Radford et al., 2019;Zhang et al., 2020;Wang et al., 2020), where offensive, unreliable and toxic content may exist (Gehman et al., 2020). Thus there are risks for these chatbots to generate responses with unsafe behavior, such as direct offensiveness, agreement to a toxic statement or harmful advice, reflecting patterns learned from the training data (Wolf et al., 2017;Nozza et al., 2021).", "publication": {"title": "RealToxi-cityPrompts: Evaluating neural toxic degeneration in language models", "authors": ["Samuel Gehman, Suchin", "Gururangan, Maarten", "Sap, Yejin", "Choi, Noah", "Smith, "], "pub_date": "2020"}}, {"citation_text": "On the right, two methods generating an alternative response are compared. 2017) trained end-to-end with Language Modeling objectives on large corpora (Radford et al., 2019;Zhang et al., 2020;Wang et al., 2020), where offensive, unreliable and toxic content may exist (Gehman et al., 2020). Thus there are risks for these chatbots to generate responses with unsafe behavior, such as direct offensiveness, agreement to a toxic statement or harmful advice, reflecting patterns learned from the training data (Wolf et al., 2017;Nozza et al., 2021).", "publication": {"title": "Why we should have seen that coming", "authors": ["Wolf, Marty J", "Miller, Keith", "Grodzinsky, Frances"], "pub_date": "2017"}}, {"citation_text": "On the right, two methods generating an alternative response are compared. 2017) trained end-to-end with Language Modeling objectives on large corpora (Radford et al., 2019;Zhang et al., 2020;Wang et al., 2020), where offensive, unreliable and toxic content may exist (Gehman et al., 2020). Thus there are risks for these chatbots to generate responses with unsafe behavior, such as direct offensiveness, agreement to a toxic statement or harmful advice, reflecting patterns learned from the training data (Wolf et al., 2017;Nozza et al., 2021).", "publication": {"title": "HONEST: Measuring Hurtful Sentence Completion in Language Models", "authors": ["Nozza, Debora", "Bianchi, Federico", "Hovy, Dirk"], "pub_date": "2021"}}, {"citation_text": "Thus there are risks for these chatbots to generate responses with unsafe behavior, such as direct offensiveness, agreement to a toxic statement or harmful advice, reflecting patterns learned from the training data (Wolf et al., 2017;Nozza et al., 2021). Current endeavors to mitigate such unsafe behavior of chatbots mainly fall on two lines: how to detect unsafe responses and how to steer conversational models towards generating safe responses.", "publication": {"title": "Why we should have seen that coming", "authors": ["Wolf, Marty J", "Miller, Keith", "Grodzinsky, Frances"], "pub_date": "2017"}}, {"citation_text": "Thus there are risks for these chatbots to generate responses with unsafe behavior, such as direct offensiveness, agreement to a toxic statement or harmful advice, reflecting patterns learned from the training data (Wolf et al., 2017;Nozza et al., 2021). Current endeavors to mitigate such unsafe behavior of chatbots mainly fall on two lines: how to detect unsafe responses and how to steer conversational models towards generating safe responses.", "publication": {"title": "HONEST: Measuring Hurtful Sentence Completion in Language Models", "authors": ["Nozza, Debora", "Bianchi, Federico", "Hovy, Dirk"], "pub_date": "2021"}}, {"citation_text": "In the first line, several related datasets with utterancelevel safety labels are proposed (Dinan et al., 2019;Baheti et al., 2021;Sun et al., 2022) to support checkers for recognition of potential unsafe utterances. However, in most cases, only some words in an utterance contribute to unsafe behavior.", "publication": {"title": "Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack", "authors": ["Dinan, Emily", "Humeau, Samuel", "Chintagunta, Bharath", "Weston, Jason"], "pub_date": "2019"}}, {"citation_text": "In the first line, several related datasets with utterancelevel safety labels are proposed (Dinan et al., 2019;Baheti et al., 2021;Sun et al., 2022) to support checkers for recognition of potential unsafe utterances. However, in most cases, only some words in an utterance contribute to unsafe behavior.", "publication": {"title": "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts", "authors": ["Baheti, Ashutosh", "Sap, Maarten", "Ritter, Alan", "Riedl, Mark"], "pub_date": "2021"}}, {"citation_text": "Along the second line, replacing detected unsafe Multi-Turn Safety-Graduated Utterance-level Safety Labels Unsafe Spans Safe Alternatives (Qian et al., 2019) Reddit + Gab \u2713 -\u2713 --ADHOMINTWEETS (Sheng et al., 2021) Twitter + Silver --\u2713 --BAD (Xu et al., 2020)", "publication": {"title": "A Benchmark Dataset for Learning to Intervene in Online Hate Speech", "authors": ["Qian, Jing", "Bethke, Anna", "Liu, Yinyin", "Belding, Elizabeth", "Wang, William"], "pub_date": "2019"}}, {"citation_text": "Along the second line, replacing detected unsafe Multi-Turn Safety-Graduated Utterance-level Safety Labels Unsafe Spans Safe Alternatives (Qian et al., 2019) Reddit + Gab \u2713 -\u2713 --ADHOMINTWEETS (Sheng et al., 2021) Twitter + Silver --\u2713 --BAD (Xu et al., 2020)", "publication": {"title": "\u201cNice Try, Kiddo\u201d: Investigating Ad Hominems in Dialogue Responses", "authors": ["Sheng, Emily", "Chang, Kai-Wei", "Natarajan, Prem", "Peng, Nanyun"], "pub_date": "2021"}}, {"citation_text": "Along the second line, replacing detected unsafe Multi-Turn Safety-Graduated Utterance-level Safety Labels Unsafe Spans Safe Alternatives (Qian et al., 2019) Reddit + Gab \u2713 -\u2713 --ADHOMINTWEETS (Sheng et al., 2021) Twitter + Silver --\u2713 --BAD (Xu et al., 2020)", "publication": {"title": "Recipes for safety in open-domain chatbots", "authors": ["Xu, Jing", "Ju, Da", "Li, Margaret", "Boureau, Y-Lan", "Weston, Jason", "Dinan, Emily"], "pub_date": "2020"}}, {"citation_text": "Reddit + Gab \u2713 -\u2713 --ADHOMINTWEETS (Sheng et al., 2021) Twitter + Silver --\u2713 --BAD (Xu et al., 2020) Human + Silver \u2713 -\u2713 --TOXICHAT (Baheti et al., 2021)", "publication": {"title": "\u201cNice Try, Kiddo\u201d: Investigating Ad Hominems in Dialogue Responses", "authors": ["Sheng, Emily", "Chang, Kai-Wei", "Natarajan, Prem", "Peng, Nanyun"], "pub_date": "2021"}}, {"citation_text": "Reddit + Gab \u2713 -\u2713 --ADHOMINTWEETS (Sheng et al., 2021) Twitter + Silver --\u2713 --BAD (Xu et al., 2020) Human + Silver \u2713 -\u2713 --TOXICHAT (Baheti et al., 2021)", "publication": {"title": "Recipes for safety in open-domain chatbots", "authors": ["Xu, Jing", "Ju, Da", "Li, Margaret", "Boureau, Y-Lan", "Weston, Jason", "Dinan, Emily"], "pub_date": "2020"}}, {"citation_text": "Reddit + Gab \u2713 -\u2713 --ADHOMINTWEETS (Sheng et al., 2021) Twitter + Silver --\u2713 --BAD (Xu et al., 2020) Human + Silver \u2713 -\u2713 --TOXICHAT (Baheti et al., 2021)", "publication": {"title": "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts", "authors": ["Baheti, Ashutosh", "Sap, Maarten", "Ritter, Alan", "Riedl, Mark"], "pub_date": "2021"}}, {"citation_text": "Human + Silver \u2713 -\u2713 --TOXICHAT (Baheti et al., 2021) Reddit + Silver --\u2713 --DIASAFETY (Sun et al., 2022)", "publication": {"title": "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts", "authors": ["Baheti, Ashutosh", "Sap, Maarten", "Ritter, Alan", "Riedl, Mark"], "pub_date": "2021"}}, {"citation_text": "Reddit + Silver --\u2713 --DIASAFETY (Sun et al., 2022) Social Media + Silver --\u2713 --SaFeRDialogues (Ung et al., 2022)", "publication": {"title": "SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures", "authors": ["Ung, Megan", "Xu, Jing", "Boureau, Y-Lan"], "pub_date": "2022"}}, {"citation_text": "Social Media + Silver --\u2713 --SaFeRDialogues (Ung et al., 2022) Human + Silver Table 1: Comparison of dialogue safety datasets.", "publication": {"title": "SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures", "authors": ["Ung, Megan", "Xu, Jing", "Boureau, Y-Lan"], "pub_date": "2022"}}, {"citation_text": "To this end, Xu et al. (2020) prepares canned responses as safe alternatives. However, the canned responses are just one of two types of safe contextual-irrelevant utterances.", "publication": {"title": "Recipes for safety in open-domain chatbots", "authors": ["Xu, Jing", "Ju, Da", "Li, Margaret", "Boureau, Y-Lan", "Weston, Jason", "Dinan, Emily"], "pub_date": "2020"}}, {"citation_text": "For unsafety detection, Qian et al. (2019), Xu et al. (2020), Baheti et al. (2021), Ung et al. (2022) and Sun et al.", "publication": {"title": "Recipes for safety in open-domain chatbots", "authors": ["Xu, Jing", "Ju, Da", "Li, Margaret", "Boureau, Y-Lan", "Weston, Jason", "Dinan, Emily"], "pub_date": "2020"}}, {"citation_text": "For unsafety detection, Qian et al. (2019), Xu et al. (2020), Baheti et al. (2021), Ung et al. (2022) and Sun et al.", "publication": {"title": "A Benchmark Dataset for Learning to Intervene in Online Hate Speech", "authors": ["Qian, Jing", "Bethke, Anna", "Liu, Yinyin", "Belding, Elizabeth", "Wang, William"], "pub_date": "2019"}}, {"citation_text": "For unsafety detection, Qian et al. (2019), Xu et al. (2020), Baheti et al. (2021), Ung et al. (2022) and Sun et al.", "publication": {"title": "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts", "authors": ["Baheti, Ashutosh", "Sap, Maarten", "Ritter, Alan", "Riedl, Mark"], "pub_date": "2021"}}, {"citation_text": "For unsafety detection, Qian et al. (2019), Xu et al. (2020), Baheti et al. (2021), Ung et al. (2022) and Sun et al.", "publication": {"title": "SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures", "authors": ["Ung, Megan", "Xu, Jing", "Boureau, Y-Lan"], "pub_date": "2022"}}, {"citation_text": "(2021), Ung et al. (2022) and Sun et al. (2022) provided utterance-level binary safety labels in their proposed dialogue datasets.", "publication": {"title": "SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures", "authors": ["Ung, Megan", "Xu, Jing", "Boureau, Y-Lan"], "pub_date": "2022"}}, {"citation_text": "Baheti et al. (2021) annotated the stance of each utterance to previous ones in the same dialogue to help unsafety detection indirectly. To steer the conversation from unsafety failures, Qian et al. (2019) and Ung et al.", "publication": {"title": "A Benchmark Dataset for Learning to Intervene in Online Hate Speech", "authors": ["Qian, Jing", "Bethke, Anna", "Liu, Yinyin", "Belding, Elizabeth", "Wang, William"], "pub_date": "2019"}}, {"citation_text": "Baheti et al. (2021) annotated the stance of each utterance to previous ones in the same dialogue to help unsafety detection indirectly. To steer the conversation from unsafety failures, Qian et al. (2019) and Ung et al.", "publication": {"title": "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts", "authors": ["Baheti, Ashutosh", "Sap, Maarten", "Ritter, Alan", "Riedl, Mark"], "pub_date": "2021"}}, {"citation_text": "To steer the conversation from unsafety failures, Qian et al. (2019) and Ung et al. (2022) rendered intervention and feedback from a third party or given by the conversation partner, respectively, in natural language that signals the occurrence of unsafety in utterances and discourages the usage of unsafe expressions.", "publication": {"title": "A Benchmark Dataset for Learning to Intervene in Online Hate Speech", "authors": ["Qian, Jing", "Bethke, Anna", "Liu, Yinyin", "Belding, Elizabeth", "Wang, William"], "pub_date": "2019"}}, {"citation_text": "To steer the conversation from unsafety failures, Qian et al. (2019) and Ung et al. (2022) rendered intervention and feedback from a third party or given by the conversation partner, respectively, in natural language that signals the occurrence of unsafety in utterances and discourages the usage of unsafe expressions.", "publication": {"title": "SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures", "authors": ["Ung, Megan", "Xu, Jing", "Boureau, Y-Lan"], "pub_date": "2022"}}, {"citation_text": "Ung et al. (2022) further required annotators to give a graceful response to acknowledge the feedback and take the conversation to an acceptable and friendly trajectory, from which chatbots could learn to recover from safe failures. However, as far as we know, SAFECONV is the first dataset with the annotation of unsafe spans and context-relevant safe alternatives.", "publication": {"title": "SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures", "authors": ["Ung, Megan", "Xu, Jing", "Boureau, Y-Lan"], "pub_date": "2022"}}, {"citation_text": "Toxicity Mitigation To detect unsafe contents, transformer-based classifiers (Devlin et al., 2019;Liu et al., 2019) are the predominant methods due to their strong representation power, upon which some datasets (Davidson et al., 2017;Hartvigsen et al., 2022) can be leveraged to train decent and powerful toxicity detectors. Finer toxicity detection, namely extracting toxic spans or phrases, can be seen as sequence labeling (Yang et al., 2018).", "publication": {"title": "", "authors": ["Devlin, Jacob", "Chang, Ming-Wei", "Lee, Kenton", "Toutanova, Kristina"], "pub_date": "2019"}}, {"citation_text": "Toxicity Mitigation To detect unsafe contents, transformer-based classifiers (Devlin et al., 2019;Liu et al., 2019) are the predominant methods due to their strong representation power, upon which some datasets (Davidson et al., 2017;Hartvigsen et al., 2022) can be leveraged to train decent and powerful toxicity detectors. Finer toxicity detection, namely extracting toxic spans or phrases, can be seen as sequence labeling (Yang et al., 2018).", "publication": {"title": "Roberta: A robustly optimized bert pretraining approach", "authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jingfei", "Joshi, Mandar", "Chen, Danqi", "Levy, Omer", "Lewis, Mike", "Zettlemoyer, Luke", "Stoyanov, Veselin"], "pub_date": "2019"}}, {"citation_text": "Toxicity Mitigation To detect unsafe contents, transformer-based classifiers (Devlin et al., 2019;Liu et al., 2019) are the predominant methods due to their strong representation power, upon which some datasets (Davidson et al., 2017;Hartvigsen et al., 2022) can be leveraged to train decent and powerful toxicity detectors. Finer toxicity detection, namely extracting toxic spans or phrases, can be seen as sequence labeling (Yang et al., 2018).", "publication": {"title": "Automated Hate Speech Detection and the Problem of Offensive Language", "authors": ["Davidson, Thomas", "Warmsley, Dana", "Macy, Michael", "Weber, Ingmar"], "pub_date": "2017"}}, {"citation_text": "Toxicity Mitigation To detect unsafe contents, transformer-based classifiers (Devlin et al., 2019;Liu et al., 2019) are the predominant methods due to their strong representation power, upon which some datasets (Davidson et al., 2017;Hartvigsen et al., 2022) can be leveraged to train decent and powerful toxicity detectors. Finer toxicity detection, namely extracting toxic spans or phrases, can be seen as sequence labeling (Yang et al., 2018).", "publication": {"title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection", "authors": ["Hartvigsen, Thomas", "Gabriel, Saadia", "Palangi, Hamid", "Sap, Maarten", "Ray, Dipankar", "Kamar, Ece"], "pub_date": "2022"}}, {"citation_text": "Toxicity Mitigation To detect unsafe contents, transformer-based classifiers (Devlin et al., 2019;Liu et al., 2019) are the predominant methods due to their strong representation power, upon which some datasets (Davidson et al., 2017;Hartvigsen et al., 2022) can be leveraged to train decent and powerful toxicity detectors. Finer toxicity detection, namely extracting toxic spans or phrases, can be seen as sequence labeling (Yang et al., 2018).", "publication": {"title": "Design challenges and misconceptions in neural sequence labeling", "authors": ["Yang, Jie", "Liang, Shuailong", "Zhang, Yue"], "pub_date": "2018"}}, {"citation_text": "Finer toxicity detection, namely extracting toxic spans or phrases, can be seen as sequence labeling (Yang et al., 2018). For text detoxification, Nogueira dos Santos et al.", "publication": {"title": "Design challenges and misconceptions in neural sequence labeling", "authors": ["Yang, Jie", "Liang, Shuailong", "Zhang, Yue"], "pub_date": "2018"}}, {"citation_text": "For text detoxification, Nogueira dos Santos et al. (2018) and Laugier et al. (2021) trained an encoderdecoder model to rewrite toxic utterances into nontoxic ones.", "publication": {"title": "Fighting Offensive Language on Social Media with Unsupervised Text Style Transfer", "authors": ["Nogueira Dos Santos, Cicero", "Melnyk, Igor", "Padhi, Inkit"], "pub_date": "2018"}}, {"citation_text": "For text detoxification, Nogueira dos Santos et al. (2018) and Laugier et al. (2021) trained an encoderdecoder model to rewrite toxic utterances into nontoxic ones.", "publication": {"title": "Civil Rephrases Of Toxic Texts With Self-Supervised Transformers", "authors": ["Laugier, L\u00e9o", "Pavlopoulos, John", "Sorensen, Jeffrey", "Dixon, Lucas"], "pub_date": "2021"}}, {"citation_text": "(2018) and Laugier et al. (2021) trained an encoderdecoder model to rewrite toxic utterances into nontoxic ones. Dathathri et al. (2020) and Krause et al.", "publication": {"title": "Civil Rephrases Of Toxic Texts With Self-Supervised Transformers", "authors": ["Laugier, L\u00e9o", "Pavlopoulos, John", "Sorensen, Jeffrey", "Dixon, Lucas"], "pub_date": "2021"}}, {"citation_text": "(2018) and Laugier et al. (2021) trained an encoderdecoder model to rewrite toxic utterances into nontoxic ones. Dathathri et al. (2020) and Krause et al.", "publication": {"title": "Plug and play language models: A simple approach to controlled text generation", "authors": ["Dathathri, Sumanth", "Madotto, Andrea", "Lan, Janice", "Hung, Jane", "Frank, Eric", "Molino, Piero", "Yosinski, Jason", "Liu, Rosanne"], "pub_date": "2020. April 26-30, 2020"}}, {"citation_text": "Dathathri et al. (2020) and Krause et al. (2021) leveraged a discriminator to constrain the language model for non-toxic generation and Dale et al.", "publication": {"title": "Plug and play language models: A simple approach to controlled text generation", "authors": ["Dathathri, Sumanth", "Madotto, Andrea", "Lan, Janice", "Hung, Jane", "Frank, Eric", "Molino, Piero", "Yosinski, Jason", "Liu, Rosanne"], "pub_date": "2020. April 26-30, 2020"}}, {"citation_text": "Dathathri et al. (2020) and Krause et al. (2021) leveraged a discriminator to constrain the language model for non-toxic generation and Dale et al.", "publication": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authors": ["Krause, Ben", "Gotmare, Akhilesh", "Mccann, Bryan", "Keskar, Nitish", "Joty, Shafiq", "Socher, Richard", "Rajani, Nazneen"], "pub_date": "2021"}}, {"citation_text": "(2021) leveraged a discriminator to constrain the language model for non-toxic generation and Dale et al. (2021) improved upon Krause et al.", "publication": {"title": "Text Detoxification using Large Pre-trained Neural Models", "authors": ["Dale, David", "Voronov, Anton", "Dementieva, Daryna", "Logacheva, Varvara", "Kozlova, Olga", "Semenov, Nikita", "Panchenko, Alexander"], "pub_date": "2021"}}, {"citation_text": "(2021) improved upon Krause et al. (2021) with a paraphrasing model for content preserving.", "publication": {"title": "GeDi: Generative Discriminator Guided Sequence Generation", "authors": ["Krause, Ben", "Gotmare, Akhilesh", "Mccann, Bryan", "Keskar, Nitish", "Joty, Shafiq", "Socher, Richard", "Rajani, Nazneen"], "pub_date": "2021"}}, {"citation_text": "Ouyang et al. (2022) and Glaese et al. (2022) injected human feedback via reinforcement learning to make the generated responses more helpful, correct, and harmless.", "publication": {"title": "Training language models to follow instructions with human feedback", "authors": ["Ouyang, Long", "Wu, Jeff", "Jiang, Xu", "Almeida, Diogo", "Wainwright, Carroll", "Mishkin, Pamela", "Zhang, Chong", "Agarwal, Sandhini", "Slama, Katarina", "Ray, Alex"], "pub_date": "2022"}}, {"citation_text": "Ouyang et al. (2022) and Glaese et al. (2022) injected human feedback via reinforcement learning to make the generated responses more helpful, correct, and harmless.", "publication": {"title": "Improving alignment of dialogue agents via targeted human judgements", "authors": ["Glaese, Amelia", "Mcaleese, Nat", "Tr\u0119bacz, Maja", "Aslanides, John", "Firoiu, Vlad", "Ewalds, Timo", "Rauh, Maribeth", "Weidinger, Laura", "Chadwick, Martin", "Thacker, Phoebe"], "pub_date": "2022"}}, {"citation_text": "To cover frequent, explicit unsafe behavior, such as explicit offensiveness, and infrequent, implicit unsafe behavior, such as agreement to harmful suggestions, we choose the dialogues of our dataset from two public large-scale conversational datasets: LCCC-base (Wang et al., 2020) and PchatbotW (Qian et al., 2021). LCCC-base contains high-quality multi-turn dialogues from Weibo which have gone through a rigorous data cleaning pipeline.", "publication": {"title": "A Large-Scale Chinese Short-Text Conversation Dataset", "authors": ["Wang, Yida", "Ke, Pei", "Zheng, Yinhe", "Huang, Kaili", "Jiang, Yong", "Zhu, Xiaoyan", "Huang, Minlie"], "pub_date": "2020"}}, {"citation_text": "To cover frequent, explicit unsafe behavior, such as explicit offensiveness, and infrequent, implicit unsafe behavior, such as agreement to harmful suggestions, we choose the dialogues of our dataset from two public large-scale conversational datasets: LCCC-base (Wang et al., 2020) and PchatbotW (Qian et al., 2021). LCCC-base contains high-quality multi-turn dialogues from Weibo which have gone through a rigorous data cleaning pipeline.", "publication": {"title": "A Large-Scale Chinese Short-Text Conversation Dataset", "authors": ["Wang, Yida", "Ke, Pei", "Zheng, Yinhe", "Huang, Kaili", "Jiang, Yong", "Zhu, Xiaoyan", "Huang, Minlie"], "pub_date": "2020"}}, {"citation_text": "To cover frequent, explicit unsafe behavior, such as explicit offensiveness, and infrequent, implicit unsafe behavior, such as agreement to harmful suggestions, we choose the dialogues of our dataset from two public large-scale conversational datasets: LCCC-base (Wang et al., 2020) and PchatbotW (Qian et al., 2021). LCCC-base contains high-quality multi-turn dialogues from Weibo which have gone through a rigorous data cleaning pipeline.", "publication": {"title": "Pchatbot: A Large-Scale Dataset for Personalized Chatbot", "authors": ["Qian, Hongjin", "Li, Xiaohe", "Zhong, Hanxun", "Guo, Yu", "Ma, Yueyuan", "Zhu, Yutao", "Liu, Zhanliang", "Dou, Zhicheng", "Wen, Ji-Rong"], "pub_date": "2021"}}, {"citation_text": "Our Jigsaw (toxicity) checker, a RoBERTa classifier (Liu et al., 2019), trained on the sampled comments achieves 88% accuracy on the test set. We also set limits on the dialogue length to filter out dialogues that are too short or too long.", "publication": {"title": "Roberta: A robustly optimized bert pretraining approach", "authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jingfei", "Joshi, Mandar", "Chen, Danqi", "Levy, Omer", "Lewis, Mike", "Zettlemoyer, Luke", "Stoyanov, Veselin"], "pub_date": "2019"}}, {"citation_text": "Our Jigsaw (toxicity) checker, a RoBERTa classifier (Liu et al., 2019), trained on the sampled comments achieves 88% accuracy on the test set. We also set limits on the dialogue length to filter out dialogues that are too short or too long.", "publication": {"title": "Roberta: A robustly optimized bert pretraining approach", "authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jingfei", "Joshi, Mandar", "Chen, Danqi", "Levy, Omer", "Lewis, Mike", "Zettlemoyer, Luke", "Stoyanov, Veselin"], "pub_date": "2019"}}, {"citation_text": "We additionally put an emphasis on the engagingness of the safe alternatives: responses that may end the conversation are avoided, such as I think you're right or Ok, which is a crucial ingredient to make a good conversation (See et al., 2019). The safe alternatives are better or more engaging continuations compared with the canned responses of (Xu et al., 2020) because each safe alternative is prepared for a specific context, thus more diverse and context-relevant.", "publication": {"title": "Recipes for safety in open-domain chatbots", "authors": ["Xu, Jing", "Ju, Da", "Li, Margaret", "Boureau, Y-Lan", "Weston, Jason", "Dinan, Emily"], "pub_date": "2020"}}, {"citation_text": "We additionally put an emphasis on the engagingness of the safe alternatives: responses that may end the conversation are avoided, such as I think you're right or Ok, which is a crucial ingredient to make a good conversation (See et al., 2019). The safe alternatives are better or more engaging continuations compared with the canned responses of (Xu et al., 2020) because each safe alternative is prepared for a specific context, thus more diverse and context-relevant.", "publication": {"title": "What makes a good conversation? How controllable attributes affect human judgments", "authors": ["See, Abigail", "Roller, Stephen", "Kiela, Douwe", "Weston, Jason"], "pub_date": "2019"}}, {"citation_text": "The safe alternatives are better or more engaging continuations compared with the canned responses of (Xu et al., 2020) because each safe alternative is prepared for a specific context, thus more diverse and context-relevant. Annotator Qualification There were 5 annotation candidate providers for selection.", "publication": {"title": "Recipes for safety in open-domain chatbots", "authors": ["Xu, Jing", "Ju, Da", "Li, Margaret", "Boureau, Y-Lan", "Weston, Jason", "Dinan, Emily"], "pub_date": "2020"}}, {"citation_text": "Our implementation is based on the Hugging-Face Transformers library (Wolf et al., 2020). Specifically, the checker is initialized as RoBERTa-base (Liu et al., 2019)  of the span are tagged as I; O denotes a word not belonging to any unsafe span.", "publication": {"title": "Roberta: A robustly optimized bert pretraining approach", "authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jingfei", "Joshi, Mandar", "Chen, Danqi", "Levy, Omer", "Lewis, Mike", "Zettlemoyer, Luke", "Stoyanov, Veselin"], "pub_date": "2019"}}, {"citation_text": "Our implementation is based on the Hugging-Face Transformers library (Wolf et al., 2020). Specifically, the checker is initialized as RoBERTa-base (Liu et al., 2019)  of the span are tagged as I; O denotes a word not belonging to any unsafe span.", "publication": {"title": "Roberta: A robustly optimized bert pretraining approach", "authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jingfei", "Joshi, Mandar", "Chen, Danqi", "Levy, Omer", "Lewis, Mike", "Zettlemoyer, Luke", "Stoyanov, Veselin"], "pub_date": "2019"}}, {"citation_text": "Our implementation is based on the Hugging-Face Transformers library (Wolf et al., 2020). Specifically, the checker is initialized as RoBERTa-base (Liu et al., 2019)  of the span are tagged as I; O denotes a word not belonging to any unsafe span.", "publication": {"title": "Transformers: State-of-the-Art Natural Language Processing", "authors": ["Wolf, Thomas", "Debut, Lysandre", "Sanh, Victor", "Chaumond, Julien", "Delangue, Clement", "Moi, Anthony", "Cistac, Pierric", "Rault, Tim", "Louf, Remi", "Funtowicz, Morgan", "Davison, Joe", "Shleifer, Sam", "Von Platen, Patrick", "Ma, Clara", "Jernite, Yacine", "Plu, Julien", "Xu, Canwen", "Le Scao, Teven", "Gugger, Sylvain", "Drame, Mariama", "Lhoest, Quentin", "Rush, Alexander"], "pub_date": "2020"}}, {"citation_text": "Specifically, the checker is initialized as RoBERTa-base (Liu et al., 2019)  of the span are tagged as I; O denotes a word not belonging to any unsafe span. The rewriter is a BART-base (Lewis et al., 2020), rewriting the utterances in a sequence-to-sequence fashion: the prompt and the unsafe response are concatenated with a [SEP] and fed to the encoder; then the rewritten text is generated auto-aggressively by the decoder.", "publication": {"title": "Roberta: A robustly optimized bert pretraining approach", "authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jingfei", "Joshi, Mandar", "Chen, Danqi", "Levy, Omer", "Lewis, Mike", "Zettlemoyer, Luke", "Stoyanov, Veselin"], "pub_date": "2019"}}, {"citation_text": "Specifically, the checker is initialized as RoBERTa-base (Liu et al., 2019)  of the span are tagged as I; O denotes a word not belonging to any unsafe span. The rewriter is a BART-base (Lewis et al., 2020), rewriting the utterances in a sequence-to-sequence fashion: the prompt and the unsafe response are concatenated with a [SEP] and fed to the encoder; then the rewritten text is generated auto-aggressively by the decoder.", "publication": {"title": "Roberta: A robustly optimized bert pretraining approach", "authors": ["Liu, Yinhan", "Ott, Myle", "Goyal, Naman", "Du, Jingfei", "Joshi, Mandar", "Chen, Danqi", "Levy, Omer", "Lewis, Mike", "Zettlemoyer, Luke", "Stoyanov, Veselin"], "pub_date": "2019"}}, {"citation_text": "Specifically, the checker is initialized as RoBERTa-base (Liu et al., 2019)  of the span are tagged as I; O denotes a word not belonging to any unsafe span. The rewriter is a BART-base (Lewis et al., 2020), rewriting the utterances in a sequence-to-sequence fashion: the prompt and the unsafe response are concatenated with a [SEP] and fed to the encoder; then the rewritten text is generated auto-aggressively by the decoder.", "publication": {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "authors": ["Lewis, Mike", "Liu, Yinhan", "Goyal, Naman", "Ghazvininejad, Marjan", "Mohamed, Abdelrahman", "Levy, Omer", "Stoyanov, Veselin", "Zettlemoyer, Luke"], "pub_date": "2020"}}, {"citation_text": "The rewriter is a BART-base (Lewis et al., 2020), rewriting the utterances in a sequence-to-sequence fashion: the prompt and the unsafe response are concatenated with a [SEP] and fed to the encoder; then the rewritten text is generated auto-aggressively by the decoder. Training Details The same configuration is used for the training of the checker, tagger, and rewriter.", "publication": {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "authors": ["Lewis, Mike", "Liu, Yinhan", "Goyal, Naman", "Ghazvininejad, Marjan", "Mohamed, Abdelrahman", "Levy, Omer", "Stoyanov, Veselin", "Zettlemoyer, Luke"], "pub_date": "2020"}}, {"citation_text": "Evaluation We compare the checker trained on SAFECONV (C SAFECONV ) with the checker trained on COLD (C COLD ) dataset (Deng et al., 2022) and the checker of Baidu 4 (C Baidu ). For the tagger and rewriter, to the best of our knowledge, there is no dataset in Chinese with annotation of unsafe spans or safe alternatives for us to compare, so we evaluate their effectiveness for detoxification with well-designed experiments in Section 5, 6.", "publication": {"title": "COLD: A Benchmark for Chinese Offensive Language Detection", "authors": ["Deng, Jiawen", "Zhou, Jingyan", "Sun, Hao", "Zheng, Chujie", "Mi, Fei", "Meng, Helen", "Huang, Minlie"], "pub_date": "2022"}}, {"citation_text": "The strategy we use to prevent the checker from seeing the unsafe spans is setting the attention weights of multi-head attention (Vaswani et al., 2017) corresponding to the unsafe spans as 05 . The results are presented in Table 6.", "publication": {"title": "Attention is all you need", "authors": ["Vaswani, Ashish", "Shazeer, Noam", "Parmar, Niki", "Uszkoreit, Jakob", "Jones, Llion", "Gomez, Aidan", "Kaiser, Lukasz", "Polosukhin, Illia"], "pub_date": "2017. 2017. December 4-9, 2017"}}, {"citation_text": "Prompts In order to increase the probability for chatbots to surface unsafe responses for rewriting, we use the Jigsaw checker (described in subsection 3.2) to search unsafe responses from 50,000 prompt-response pairs from LCCC-large (Wang et al., 2020) and 50,000 from PChatbotW (Qian et al., 2021) and only keep their prompts.", "publication": {"title": "A Large-Scale Chinese Short-Text Conversation Dataset", "authors": ["Wang, Yida", "Ke, Pei", "Zheng, Yinhe", "Huang, Kaili", "Jiang, Yong", "Zhu, Xiaoyan", "Huang, Minlie"], "pub_date": "2020"}}, {"citation_text": "Prompts In order to increase the probability for chatbots to surface unsafe responses for rewriting, we use the Jigsaw checker (described in subsection 3.2) to search unsafe responses from 50,000 prompt-response pairs from LCCC-large (Wang et al., 2020) and 50,000 from PChatbotW (Qian et al., 2021) and only keep their prompts.", "publication": {"title": "A Large-Scale Chinese Short-Text Conversation Dataset", "authors": ["Wang, Yida", "Ke, Pei", "Zheng, Yinhe", "Huang, Kaili", "Jiang, Yong", "Zhu, Xiaoyan", "Huang, Minlie"], "pub_date": "2020"}}, {"citation_text": "Prompts In order to increase the probability for chatbots to surface unsafe responses for rewriting, we use the Jigsaw checker (described in subsection 3.2) to search unsafe responses from 50,000 prompt-response pairs from LCCC-large (Wang et al., 2020) and 50,000 from PChatbotW (Qian et al., 2021) and only keep their prompts.", "publication": {"title": "Pchatbot: A Large-Scale Dataset for Personalized Chatbot", "authors": ["Qian, Hongjin", "Li, Xiaohe", "Zhong, Hanxun", "Guo, Yu", "Ma, Yueyuan", "Zhu, Yutao", "Liu, Zhanliang", "Dou, Zhicheng", "Wen, Ji-Rong"], "pub_date": "2021"}}, {"citation_text": "(Qian et al., 2021) and only keep their prompts. We get 14,632 prompts in total.", "publication": {"title": "Pchatbot: A Large-Scale Dataset for Personalized Chatbot", "authors": ["Qian, Hongjin", "Li, Xiaohe", "Zhong, Hanxun", "Guo, Yu", "Ma, Yueyuan", "Zhu, Yutao", "Liu, Zhanliang", "Dou, Zhicheng", "Wen, Ji-Rong"], "pub_date": "2021"}}, {"citation_text": "CDialGPTbase (Wang et al., 2020), a decoder-based chatbot with 95.5M parameters, is trained with a large corpus of conversations collected mainly from Weibo comments. Different from CDialGPT-base, CDialGPT-large is trained with more dialogues from a mixup of multiple data sources.", "publication": {"title": "A Large-Scale Chinese Short-Text Conversation Dataset", "authors": ["Wang, Yida", "Ke, Pei", "Zheng, Yinhe", "Huang, Kaili", "Jiang, Yong", "Zhu, Xiaoyan", "Huang, Minlie"], "pub_date": "2020"}}, {"citation_text": "CDialGPTbase (Wang et al., 2020), a decoder-based chatbot with 95.5M parameters, is trained with a large corpus of conversations collected mainly from Weibo comments. Different from CDialGPT-base, CDialGPT-large is trained with more dialogues from a mixup of multiple data sources.", "publication": {"title": "A Large-Scale Chinese Short-Text Conversation Dataset", "authors": ["Wang, Yida", "Ke, Pei", "Zheng, Yinhe", "Huang, Kaili", "Jiang, Yong", "Zhu, Xiaoyan", "Huang, Minlie"], "pub_date": "2020"}}, {"citation_text": "EVAbase (Gu et al., 2022) is a encoder-decoder-based conversational model with 300M parameters pretrained on cleaned WDC-Dialogue (Zhou et al., 2021). Different from EVA-base, EVA-large has a larger scale of 970M parameters.", "publication": {"title": "Eva: An opendomain chinese dialogue system with large-scale generative pre-training", "authors": ["Zhou, Hao", "Ke, Pei", "Zhang, Zheng", "Gu, Yuxian", "Zheng, Yinhe", "Zheng, Chujie", "Wang, Yida", "Wu, Chen", "Sun, Hao", "Yang, Xiaocong"], "pub_date": "2021"}}, {"citation_text": "We are interested in the CDialGPT-base (Wang et al., 2020) 95.5M 484.0 174.5 (63.9% \u21d3) 85.0 (82.4% \u21d3) CDialGPT-large (Wang et al., 2020) 95.5M 439.8 176.0 (60.0% \u21d3) 89.0 (79.8% \u21d3) EVA-base (Gu et al.,  question: can we further improve the rewriter by making it aware of its bad generations?", "publication": {"title": "A Large-Scale Chinese Short-Text Conversation Dataset", "authors": ["Wang, Yida", "Ke, Pei", "Zheng, Yinhe", "Huang, Kaili", "Jiang, Yong", "Zhu, Xiaoyan", "Huang, Minlie"], "pub_date": "2020"}}, {"citation_text": "We are interested in the CDialGPT-base (Wang et al., 2020) 95.5M 484.0 174.5 (63.9% \u21d3) 85.0 (82.4% \u21d3) CDialGPT-large (Wang et al., 2020) 95.5M 439.8 176.0 (60.0% \u21d3) 89.0 (79.8% \u21d3) EVA-base (Gu et al.,  question: can we further improve the rewriter by making it aware of its bad generations?", "publication": {"title": "A Large-Scale Chinese Short-Text Conversation Dataset", "authors": ["Wang, Yida", "Ke, Pei", "Zheng, Yinhe", "Huang, Kaili", "Jiang, Yong", "Zhu, Xiaoyan", "Huang, Minlie"], "pub_date": "2020"}}, {"citation_text": "95.5M 439.8 176.0 (60.0% \u21d3) 89.0 (79.8% \u21d3) EVA-base (Gu et al.,  question: can we further improve the rewriter by making it aware of its bad generations? We further finetune the rewriter on the feedback of the safety checker with PPO (Schulman et al., 2017;Ouyang et al., 2022), a policy optimization method in Reinforcement Learning (RL).", "publication": {"title": "Proximal policy optimization algorithms", "authors": ["Schulman, John", "Wolski, Filip", "Dhariwal, Prafulla", "Radford, Alec", "Klimov, Oleg"], "pub_date": "2017"}}, {"citation_text": "95.5M 439.8 176.0 (60.0% \u21d3) 89.0 (79.8% \u21d3) EVA-base (Gu et al.,  question: can we further improve the rewriter by making it aware of its bad generations? We further finetune the rewriter on the feedback of the safety checker with PPO (Schulman et al., 2017;Ouyang et al., 2022), a policy optimization method in Reinforcement Learning (RL).", "publication": {"title": "Training language models to follow instructions with human feedback", "authors": ["Ouyang, Long", "Wu, Jeff", "Jiang, Xu", "Almeida, Diogo", "Wainwright, Carroll", "Mishkin, Pamela", "Zhang, Chong", "Agarwal, Sandhini", "Slama, Katarina", "Ray, Alex"], "pub_date": "2022"}}, {"citation_text": "We further finetune the rewriter on the feedback of the safety checker with PPO (Schulman et al., 2017;Ouyang et al., 2022), a policy optimization method in Reinforcement Learning (RL). Specifically, the objective to optimize is: where \u03b8 and \u03b8 \u2032 are the parameters of the rewriter to optimize and before finetuning; x, y and y \u2032 denote the prompt, response and rewritten response.", "publication": {"title": "Proximal policy optimization algorithms", "authors": ["Schulman, John", "Wolski, Filip", "Dhariwal, Prafulla", "Radford, Alec", "Klimov, Oleg"], "pub_date": "2017"}}, {"citation_text": "We further finetune the rewriter on the feedback of the safety checker with PPO (Schulman et al., 2017;Ouyang et al., 2022), a policy optimization method in Reinforcement Learning (RL). Specifically, the objective to optimize is: where \u03b8 and \u03b8 \u2032 are the parameters of the rewriter to optimize and before finetuning; x, y and y \u2032 denote the prompt, response and rewritten response.", "publication": {"title": "Training language models to follow instructions with human feedback", "authors": ["Ouyang, Long", "Wu, Jeff", "Jiang, Xu", "Almeida, Diogo", "Wainwright, Carroll", "Mishkin, Pamela", "Zhang, Chong", "Agarwal, Sandhini", "Slama, Katarina", "Ray, Alex"], "pub_date": "2022"}}, {"citation_text": "Similar to Ouyang et al. (2022) , we add KL penalty from the rewriter before finetuning at the model distribution of each token to avoid over-optimization and set \u03b2 as 0.02.", "publication": {"title": "Training language models to follow instructions with human feedback", "authors": ["Ouyang, Long", "Wu, Jeff", "Jiang, Xu", "Almeida, Diogo", "Wainwright, Carroll", "Mishkin, Pamela", "Zhang, Chong", "Agarwal, Sandhini", "Slama, Katarina", "Ray, Alex"], "pub_date": "2022"}}, {"citation_text": "Besides, there is no leakage of personal information because our data sources, LCCC-base (Wang et al., 2020) and PchatbotW (Qian et al., 2021) have already been preprocessed to remove personal information by researchers of previous work (see their papers for details). Also, though our dataset contains more instances compared to previously proposed datasets, the dialogues are mostly from social media and may not cover types of conversational unsafe behavior found in other media.", "publication": {"title": "A Large-Scale Chinese Short-Text Conversation Dataset", "authors": ["Wang, Yida", "Ke, Pei", "Zheng, Yinhe", "Huang, Kaili", "Jiang, Yong", "Zhu, Xiaoyan", "Huang, Minlie"], "pub_date": "2020"}}, {"citation_text": "Besides, there is no leakage of personal information because our data sources, LCCC-base (Wang et al., 2020) and PchatbotW (Qian et al., 2021) have already been preprocessed to remove personal information by researchers of previous work (see their papers for details). Also, though our dataset contains more instances compared to previously proposed datasets, the dialogues are mostly from social media and may not cover types of conversational unsafe behavior found in other media.", "publication": {"title": "A Large-Scale Chinese Short-Text Conversation Dataset", "authors": ["Wang, Yida", "Ke, Pei", "Zheng, Yinhe", "Huang, Kaili", "Jiang, Yong", "Zhu, Xiaoyan", "Huang, Minlie"], "pub_date": "2020"}}, {"citation_text": "Besides, there is no leakage of personal information because our data sources, LCCC-base (Wang et al., 2020) and PchatbotW (Qian et al., 2021) have already been preprocessed to remove personal information by researchers of previous work (see their papers for details). Also, though our dataset contains more instances compared to previously proposed datasets, the dialogues are mostly from social media and may not cover types of conversational unsafe behavior found in other media.", "publication": {"title": "Pchatbot: A Large-Scale Dataset for Personalized Chatbot", "authors": ["Qian, Hongjin", "Li, Xiaohe", "Zhong, Hanxun", "Guo, Yu", "Ma, Yueyuan", "Zhu, Yutao", "Liu, Zhanliang", "Dou, Zhicheng", "Wen, Ji-Rong"], "pub_date": "2021"}}]}