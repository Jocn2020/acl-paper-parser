{"publication_detail": {"title": "Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking", "authors": ["Bebensee, Bj\u00f6rn", "Lee, Haejun"], "pub_date": ""}, "abstract": "\nIn schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and Mul-tiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30\u00d7 larger D3ST-XXL model by 5.0 points.\n", "citations": [{"citation_text": "Dialogue State Tracking (DST) refers to the task of estimating and tracking the dialogue state consisting of the user's current intent and set of slotvalue pairs throughout the dialogue (Williams et al., 2013). Traditional approaches to DST assume a fixed ontology and learn a classifier for each slot (Chao and Lane, 2019).", "publication": {"title": "The dialog state tracking challenge", "authors": ["Williams, Jason", "Raux, Antoine", "Ramachandran, Deepak", "Black, Alan"], "pub_date": "2013"}}, {"citation_text": "Recently more flexible schemaguided approaches which take as input natural language descriptions of all available intents and slots and thus can be applied zero-shot to new services have been gaining popularity (Rastogi et al., 2020;Feng et al., 2021;Zhao et al., 2022;Gupta et al., 2022). Discriminative DST models are based on machine reading comprehension (MRC) methods, meaning they extract and fill in non-categorical slot values directly from the user utterances (Chao and Lane, 2019;Ruan et al., 2020;Zhang et al., 2021).", "publication": {"title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset", "authors": ["Rastogi, Abhinav", "Zang, Xiaoxue", "Sunkara, Srinivas", "Gupta, Raghav", "Khaitan, Pranav"], "pub_date": "2020"}}, {"citation_text": "Recently more flexible schemaguided approaches which take as input natural language descriptions of all available intents and slots and thus can be applied zero-shot to new services have been gaining popularity (Rastogi et al., 2020;Feng et al., 2021;Zhao et al., 2022;Gupta et al., 2022). Discriminative DST models are based on machine reading comprehension (MRC) methods, meaning they extract and fill in non-categorical slot values directly from the user utterances (Chao and Lane, 2019;Ruan et al., 2020;Zhang et al., 2021).", "publication": {"title": "A sequenceto-sequence approach to dialogue state tracking", "authors": ["Feng, Yue", "Wang, Yang", "Li, Hang"], "pub_date": "2021"}}, {"citation_text": "Recently more flexible schemaguided approaches which take as input natural language descriptions of all available intents and slots and thus can be applied zero-shot to new services have been gaining popularity (Rastogi et al., 2020;Feng et al., 2021;Zhao et al., 2022;Gupta et al., 2022). Discriminative DST models are based on machine reading comprehension (MRC) methods, meaning they extract and fill in non-categorical slot values directly from the user utterances (Chao and Lane, 2019;Ruan et al., 2020;Zhang et al., 2021).", "publication": {"title": "Descriptiondriven task-oriented dialog modeling", "authors": ["Zhao, Jeffrey", "Gupta, Raghav", "Cao, Yuan", "Yu, Dian", "Wang, Mingqiu", "Lee, Harrison", "Rastogi, Abhinav", "Shafran, Izhak", "Wu, Yonghui"], "pub_date": "2022"}}, {"citation_text": "Recently more flexible schemaguided approaches which take as input natural language descriptions of all available intents and slots and thus can be applied zero-shot to new services have been gaining popularity (Rastogi et al., 2020;Feng et al., 2021;Zhao et al., 2022;Gupta et al., 2022). Discriminative DST models are based on machine reading comprehension (MRC) methods, meaning they extract and fill in non-categorical slot values directly from the user utterances (Chao and Lane, 2019;Ruan et al., 2020;Zhang et al., 2021).", "publication": {"title": "Show, Don\u2019t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue", "authors": ["Gupta, Raghav", "Lee, Harrison", "Zhao, Jeffrey", "Cao, Yuan", "Rastogi, Abhinav", "Wu, Yonghui"], "pub_date": "2022"}}, {"citation_text": "Recently more flexible schemaguided approaches which take as input natural language descriptions of all available intents and slots and thus can be applied zero-shot to new services have been gaining popularity (Rastogi et al., 2020;Feng et al., 2021;Zhao et al., 2022;Gupta et al., 2022). Discriminative DST models are based on machine reading comprehension (MRC) methods, meaning they extract and fill in non-categorical slot values directly from the user utterances (Chao and Lane, 2019;Ruan et al., 2020;Zhang et al., 2021).", "publication": {"title": "Fine-tuning bert for schema-guided zero-shot dialogue state tracking", "authors": ["Ruan, Yu-Ping", "Ling, Zhen-Hua", "Gu, Jia-Chen", "Liu, Quan"], "pub_date": "2020"}}, {"citation_text": "Recently more flexible schemaguided approaches which take as input natural language descriptions of all available intents and slots and thus can be applied zero-shot to new services have been gaining popularity (Rastogi et al., 2020;Feng et al., 2021;Zhao et al., 2022;Gupta et al., 2022). Discriminative DST models are based on machine reading comprehension (MRC) methods, meaning they extract and fill in non-categorical slot values directly from the user utterances (Chao and Lane, 2019;Ruan et al., 2020;Zhang et al., 2021).", "publication": {"title": "Sgd-qa: Fast schema-guided dialogue state tracking for unseen services", "authors": ["Zhang, Yang", "Noroozi, Vahid", "Bakhturina, Evelina", "Ginsburg, Boris"], "pub_date": "2021"}}, {"citation_text": "Discriminative DST models are based on machine reading comprehension (MRC) methods, meaning they extract and fill in non-categorical slot values directly from the user utterances (Chao and Lane, 2019;Ruan et al., 2020;Zhang et al., 2021). We use the terms discriminative and extractive interchangeably when referring to these methods.", "publication": {"title": "Fine-tuning bert for schema-guided zero-shot dialogue state tracking", "authors": ["Ruan, Yu-Ping", "Ling, Zhen-Hua", "Gu, Jia-Chen", "Liu, Quan"], "pub_date": "2020"}}, {"citation_text": "Discriminative DST models are based on machine reading comprehension (MRC) methods, meaning they extract and fill in non-categorical slot values directly from the user utterances (Chao and Lane, 2019;Ruan et al., 2020;Zhang et al., 2021). We use the terms discriminative and extractive interchangeably when referring to these methods.", "publication": {"title": "Sgd-qa: Fast schema-guided dialogue state tracking for unseen services", "authors": ["Zhang, Yang", "Noroozi, Vahid", "Bakhturina, Evelina", "Ginsburg, Boris"], "pub_date": "2021"}}, {"citation_text": "Prior generative methods do not generalize well to variations in schema (Lee et al., 2021(Lee et al., , 2022;;Zhao et al., 2022) whereas discriminative methods separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. In this work we introduce the SPan-Selective Linear Attention Transformer, short SPLAT, a novel architecture designed to achieve better generalization, robustness and efficiency in DST than existing approaches.", "publication": {"title": "Dialogue state tracking with a language model using schema-driven prompting", "authors": ["Lee, Chia-Hsuan", "Cheng, Hao", "Ostendorf, Mari"], "pub_date": "2021"}}, {"citation_text": "Prior generative methods do not generalize well to variations in schema (Lee et al., 2021(Lee et al., , 2022;;Zhao et al., 2022) whereas discriminative methods separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. In this work we introduce the SPan-Selective Linear Attention Transformer, short SPLAT, a novel architecture designed to achieve better generalization, robustness and efficiency in DST than existing approaches.", "publication": {"title": "Sgd-x: A benchmark for robust generalization in schemaguided dialogue systems", "authors": ["Lee, Harrison", "Gupta, Raghav", "Rastogi, Abhinav", "Cao, Yuan", "Zhang, Bin", "Wu, Yonghui"], "pub_date": "2022"}}, {"citation_text": "Prior generative methods do not generalize well to variations in schema (Lee et al., 2021(Lee et al., , 2022;;Zhao et al., 2022) whereas discriminative methods separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. In this work we introduce the SPan-Selective Linear Attention Transformer, short SPLAT, a novel architecture designed to achieve better generalization, robustness and efficiency in DST than existing approaches.", "publication": {"title": "Descriptiondriven task-oriented dialog modeling", "authors": ["Zhao, Jeffrey", "Gupta, Raghav", "Cao, Yuan", "Yu, Dian", "Wang, Mingqiu", "Lee, Harrison", "Rastogi, Abhinav", "Shafran, Izhak", "Wu, Yonghui"], "pub_date": "2022"}}, {"citation_text": "Then we take a contrastive query-based pointer network approach (Vinyals et al., 2015) to match special query tokens to the target slot value's learned span representation in a single pass. Our main contributions are as follows: \u2022 We propose novel span-selective prediction layers for DST which provide better generalization and efficiency by limiting the prediction space and inferring all predictions in parallel.", "publication": {"title": "Pointer networks. Advances in neural information processing systems", "authors": ["Vinyals, Oriol", "Fortunato, Meire", "Jaitly, Navdeep"], "pub_date": "2015"}}, {"citation_text": "In order to better capture the semantics of the input and to al-low for a longer context as well as all the relevant schema descriptions to be encoded jointly we use a Transformer (Vaswani et al., 2017) with linear-time attention. Instead of computing the full attention matrix as the original Transformer does, its linear attention variants compute either an approximation of it (Choromanski et al., 2021) or only compute full attention for a fixed context window of size w around the current token and additional n global global tokens, thus lowering the complexity of the attention computation from O(n 2 ) for a sequence of length n to O(w + n global ) (Beltagy et al., 2020;Zaheer et al., 2020).", "publication": {"title": "Attention is all you need", "authors": ["Vaswani, Ashish", "Shazeer, Noam", "Parmar, Niki", "Uszkoreit, Jakob", "Jones, Llion", "Gomez, Aidan", "Kaiser, \u0141ukasz", "Polosukhin, Illia"], "pub_date": "2017"}}, {"citation_text": "In order to better capture the semantics of the input and to al-low for a longer context as well as all the relevant schema descriptions to be encoded jointly we use a Transformer (Vaswani et al., 2017) with linear-time attention. Instead of computing the full attention matrix as the original Transformer does, its linear attention variants compute either an approximation of it (Choromanski et al., 2021) or only compute full attention for a fixed context window of size w around the current token and additional n global global tokens, thus lowering the complexity of the attention computation from O(n 2 ) for a sequence of length n to O(w + n global ) (Beltagy et al., 2020;Zaheer et al., 2020).", "publication": {"title": "Rethinking attention with performers", "authors": ["Krzysztof Marcin Choromanski, Valerii", "Likhosherstov, David", "Dohan, Xingyou", "Song, Andreea", "Gane, Tamas", "Sarlos, Peter", "Hawkins, Jared", "Davis, Afroz", "Mohiuddin, Lukasz", "Kaiser, David", "Belanger, Lucy", "Colwell, Adrian", "Weller, "], "pub_date": "2021"}}, {"citation_text": "In order to better capture the semantics of the input and to al-low for a longer context as well as all the relevant schema descriptions to be encoded jointly we use a Transformer (Vaswani et al., 2017) with linear-time attention. Instead of computing the full attention matrix as the original Transformer does, its linear attention variants compute either an approximation of it (Choromanski et al., 2021) or only compute full attention for a fixed context window of size w around the current token and additional n global global tokens, thus lowering the complexity of the attention computation from O(n 2 ) for a sequence of length n to O(w + n global ) (Beltagy et al., 2020;Zaheer et al., 2020).", "publication": {"title": "Preprint repository arXiv achieves milestone million uploads", "authors": ["Beltagy, Iz", "Peters, Matthew", "Cohan, Arman"], "pub_date": "2020"}}, {"citation_text": "In order to better capture the semantics of the input and to al-low for a longer context as well as all the relevant schema descriptions to be encoded jointly we use a Transformer (Vaswani et al., 2017) with linear-time attention. Instead of computing the full attention matrix as the original Transformer does, its linear attention variants compute either an approximation of it (Choromanski et al., 2021) or only compute full attention for a fixed context window of size w around the current token and additional n global global tokens, thus lowering the complexity of the attention computation from O(n 2 ) for a sequence of length n to O(w + n global ) (Beltagy et al., 2020;Zaheer et al., 2020).", "publication": {"title": "Big bird: Transformers for longer sequences", "authors": ["Zaheer, Manzil", "Guruganesh, Guru", "Kumar, Avinava", "Dubey, Joshua", "Ainslie, Chris", "Alberti, Santiago", "Ontanon, Philip", "Pham, Anirudh", "Ravula, Qifan", "Wang, Li", "Yang, "], "pub_date": "2020"}}, {"citation_text": "Instead of computing the full attention matrix as the original Transformer does, its linear attention variants compute either an approximation of it (Choromanski et al., 2021) or only compute full attention for a fixed context window of size w around the current token and additional n global global tokens, thus lowering the complexity of the attention computation from O(n 2 ) for a sequence of length n to O(w + n global ) (Beltagy et al., 2020;Zaheer et al., 2020). We focus on the windowed variant and incorporate it to DST.", "publication": {"title": "Rethinking attention with performers", "authors": ["Krzysztof Marcin Choromanski, Valerii", "Likhosherstov, David", "Dohan, Xingyou", "Song, Andreea", "Gane, Tamas", "Sarlos, Peter", "Hawkins, Jared", "Davis, Afroz", "Mohiuddin, Lukasz", "Kaiser, David", "Belanger, Lucy", "Colwell, Adrian", "Weller, "], "pub_date": "2021"}}, {"citation_text": "Instead of computing the full attention matrix as the original Transformer does, its linear attention variants compute either an approximation of it (Choromanski et al., 2021) or only compute full attention for a fixed context window of size w around the current token and additional n global global tokens, thus lowering the complexity of the attention computation from O(n 2 ) for a sequence of length n to O(w + n global ) (Beltagy et al., 2020;Zaheer et al., 2020). We focus on the windowed variant and incorporate it to DST.", "publication": {"title": "Preprint repository arXiv achieves milestone million uploads", "authors": ["Beltagy, Iz", "Peters, Matthew", "Cohan, Arman"], "pub_date": "2020"}}, {"citation_text": "Instead of computing the full attention matrix as the original Transformer does, its linear attention variants compute either an approximation of it (Choromanski et al., 2021) or only compute full attention for a fixed context window of size w around the current token and additional n global global tokens, thus lowering the complexity of the attention computation from O(n 2 ) for a sequence of length n to O(w + n global ) (Beltagy et al., 2020;Zaheer et al., 2020). We focus on the windowed variant and incorporate it to DST.", "publication": {"title": "Big bird: Transformers for longer sequences", "authors": ["Zaheer, Manzil", "Guruganesh, Guru", "Kumar, Avinava", "Dubey, Joshua", "Ainslie, Chris", "Alberti, Santiago", "Ontanon, Philip", "Pham, Anirudh", "Ravula, Qifan", "Wang, Li", "Yang, "], "pub_date": "2020"}}, {"citation_text": "While we choose the Longformer (Beltagy et al., 2020) for our implementation, in practice any variants with windowed and global attention can be used instead. Joint encoding.", "publication": {"title": "Preprint repository arXiv achieves milestone million uploads", "authors": ["Beltagy, Iz", "Peters, Matthew", "Cohan, Arman"], "pub_date": "2020"}}, {"citation_text": "We introduce a novel Span Pointer Module which computes span representations via a span encoder and extracts slot values by matching slot queries via a similarity-based span pointing mechanism (Vinyals et al., 2015). First, for any given span of token representations x i , .", "publication": {"title": "Pointer networks. Advances in neural information processing systems", "authors": ["Vinyals, Oriol", "Fortunato, Meire", "Jaitly, Navdeep"], "pub_date": "2015"}}, {"citation_text": ", x j in the joint encoding E we obtain the span representation h SPAN ij by concatenating the span's first and last token representation and feeding them into a 2-layer feed-forward span encoder (Joshi et al., 2020): Similarly, for each slot token representation x [SLOT] in E we compute a slot query representation h [SLOT] with a 2-layer feed-forward slot encoder: (5) Given slots S = {s 1 , . .", "publication": {"title": "Spanbert: Improving pre-training by representing and predicting spans", "authors": ["Joshi, Mandar", "Chen, Danqi", "Liu, Yinhan", "Daniel S Weld, Luke", "Zettlemoyer, Omer", "Levy, "], "pub_date": "2020"}}, {"citation_text": "In order to improve span representations for down-stream applications to DST we pre-train SPLAT in a self-supervised manner using a modified recurrent span selection objective (Ram et al., 2021). Given an input text I let R = {R 1 , .", "publication": {"title": "Few-shot question answering by pretraining span selection", "authors": ["Ram, Ori", "Kirstain, Yuval", "Berant, Jonathan", "Globerson, Amir", "Levy, Omer"], "pub_date": "2021"}}, {"citation_text": "Following Ram et al. (2021) we randomly select a subset M \u2286 R of J recurring spans such that the number of their occurrences sums up to a maximum of 30 occurrences.", "publication": {"title": "Few-shot question answering by pretraining span selection", "authors": ["Ram, Ori", "Kirstain, Yuval", "Berant, Jonathan", "Globerson, Amir", "Levy, Omer"], "pub_date": "2021"}}, {"citation_text": "We conduct experiments on the Schema-Guided Dialogue (SGD) (Rastogi et al., 2020), SGD-X (Lee et al., 2022) and MultiWOZ 2.2 (Zang et al., 2020) datasets. Schema-Guided Dialogue.", "publication": {"title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset", "authors": ["Rastogi, Abhinav", "Zang, Xiaoxue", "Sunkara, Srinivas", "Gupta, Raghav", "Khaitan, Pranav"], "pub_date": "2020"}}, {"citation_text": "We conduct experiments on the Schema-Guided Dialogue (SGD) (Rastogi et al., 2020), SGD-X (Lee et al., 2022) and MultiWOZ 2.2 (Zang et al., 2020) datasets. Schema-Guided Dialogue.", "publication": {"title": "Sgd-x: A benchmark for robust generalization in schemaguided dialogue systems", "authors": ["Lee, Harrison", "Gupta, Raghav", "Rastogi, Abhinav", "Cao, Yuan", "Zhang, Bin", "Wu, Yonghui"], "pub_date": "2022"}}, {"citation_text": "We conduct experiments on the Schema-Guided Dialogue (SGD) (Rastogi et al., 2020), SGD-X (Lee et al., 2022) and MultiWOZ 2.2 (Zang et al., 2020) datasets. Schema-Guided Dialogue.", "publication": {"title": "MultiWOZ 2.2 : A dialogue dataset with additional annotation corrections and state tracking baselines", "authors": ["Zang, Xiaoxue", "Rastogi, Abhinav", "Sunkara, Srinivas", "Gupta, Raghav", "Zhang, Jianguo", "Chen, Jindong"], "pub_date": "2020"}}, {"citation_text": "There are multiple updated versions of the original Mul-tiWOZ dataset (Budzianowski et al., 2018): Mul-tiWOZ 2.1 (Eric et al., 2020) and MultiWOZ 2.2 (Zang et al., 2020) fix annotation errors of previous versions, MultiWOZ 2.3 (Han et al., 2021) is based on version 2.1 and adds co-reference annotations, MultiWOZ 2.4 (Ye et al., 2022) is also based on version 2.1 and includes test set corrections. However, MultiWOZ 2.2 is the only version of the dataset which includes a fully defined schema matching the ontology.", "publication": {"title": "MultiWOZ 2.2 : A dialogue dataset with additional annotation corrections and state tracking baselines", "authors": ["Zang, Xiaoxue", "Rastogi, Abhinav", "Sunkara, Srinivas", "Gupta, Raghav", "Zhang, Jianguo", "Chen, Jindong"], "pub_date": "2020"}}, {"citation_text": "There are multiple updated versions of the original Mul-tiWOZ dataset (Budzianowski et al., 2018): Mul-tiWOZ 2.1 (Eric et al., 2020) and MultiWOZ 2.2 (Zang et al., 2020) fix annotation errors of previous versions, MultiWOZ 2.3 (Han et al., 2021) is based on version 2.1 and adds co-reference annotations, MultiWOZ 2.4 (Ye et al., 2022) is also based on version 2.1 and includes test set corrections. However, MultiWOZ 2.2 is the only version of the dataset which includes a fully defined schema matching the ontology.", "publication": {"title": "MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling", "authors": ["Budzianowski, Pawe\u0142", "Wen, Tsung-Hsien", "Tseng, Bo-Hsiang", "Casanueva, I\u00f1igo", "Ultes, Stefan", "Ramadan, Osman", "Ga\u0161i\u0107, Milica"], "pub_date": "2018"}}, {"citation_text": "There are multiple updated versions of the original Mul-tiWOZ dataset (Budzianowski et al., 2018): Mul-tiWOZ 2.1 (Eric et al., 2020) and MultiWOZ 2.2 (Zang et al., 2020) fix annotation errors of previous versions, MultiWOZ 2.3 (Han et al., 2021) is based on version 2.1 and adds co-reference annotations, MultiWOZ 2.4 (Ye et al., 2022) is also based on version 2.1 and includes test set corrections. However, MultiWOZ 2.2 is the only version of the dataset which includes a fully defined schema matching the ontology.", "publication": {"title": "Mul-tiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines", "authors": ["Eric, Mihail", "Goel, Rahul", "Paul, Shachi", "Sethi, Abhishek", "Agarwal, Sanchit", "Gao, Shuyang", "Kumar, Adarsh", "Goyal, Anuj", "Ku, Peter", "Hakkani-Tur, Dilek"], "pub_date": "2020"}}, {"citation_text": "There are multiple updated versions of the original Mul-tiWOZ dataset (Budzianowski et al., 2018): Mul-tiWOZ 2.1 (Eric et al., 2020) and MultiWOZ 2.2 (Zang et al., 2020) fix annotation errors of previous versions, MultiWOZ 2.3 (Han et al., 2021) is based on version 2.1 and adds co-reference annotations, MultiWOZ 2.4 (Ye et al., 2022) is also based on version 2.1 and includes test set corrections. However, MultiWOZ 2.2 is the only version of the dataset which includes a fully defined schema matching the ontology.", "publication": {"title": "Multiwoz 2.3: A multi-domain task-oriented dialogue dataset enhanced with annotation corrections and co-reference annotation", "authors": ["Han, Ting", "Liu, Ximing", "Takanabu, Ryuichi", "Lian, Yixin", "Huang, Chongxuan", "Wan, Dazhen", "Peng, Wei", "Huang, Minlie"], "pub_date": "2021"}}, {"citation_text": "There are multiple updated versions of the original Mul-tiWOZ dataset (Budzianowski et al., 2018): Mul-tiWOZ 2.1 (Eric et al., 2020) and MultiWOZ 2.2 (Zang et al., 2020) fix annotation errors of previous versions, MultiWOZ 2.3 (Han et al., 2021) is based on version 2.1 and adds co-reference annotations, MultiWOZ 2.4 (Ye et al., 2022) is also based on version 2.1 and includes test set corrections. However, MultiWOZ 2.2 is the only version of the dataset which includes a fully defined schema matching the ontology.", "publication": {"title": "MultiWOZ 2.4: A multi-domain task-oriented dialogue dataset with essential annotation corrections to improve state tracking evaluation", "authors": ["Ye, Fanghua", "Manotumruksa, Jarana", "Yilmaz, Emine"], "pub_date": "2022"}}, {"citation_text": "In line with prior work (Rastogi et al., 2020) we evaluate our approach according to the following two metrics. Intent Accuracy: For intent detection the intent accuracy describes the fraction of turns for which the active intent has been correctly inferred.", "publication": {"title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset", "authors": ["Rastogi, Abhinav", "Zang, Xiaoxue", "Sunkara, Srinivas", "Gupta, Raghav", "Khaitan, Pranav"], "pub_date": "2020"}}, {"citation_text": "We base our implementation on the Longformer code included in the HuggingFace Transformers library (Wolf et al., 2020) and continue training from the base model (110M parameters) and large model (340M parameters) checkpoints. We keep the default Longformer hyperparameters in place, in particular we keep the attention window size set to 512.", "publication": {"title": "Transformers: State-of-the-art natural language processing", "authors": ["Wolf, Thomas", "Debut, Lysandre", "Sanh, Victor", "Chaumond, Julien", "Delangue, Clement", "Moi, Anthony", "Cistac, Pierric", "Rault, Tim", "Louf, R\u00e9mi", "Funtowicz, Morgan", "Davison, Joe", "Shleifer, Sam", "Patrick Von Platen, Clara", "Ma, Yacine", "Jernite, Julien", "Plu, Canwen", "Xu, Teven", "Le Scao, Sylvain", "Gugger, Mariama", "Drame, Quentin", "Lhoest, Alexander", "Rush, "], "pub_date": "2020"}}, {"citation_text": "Specifically we use the KILT Wikipedia snapshot 1 from 2019 (Petroni et al., 2021) as provided by the HuggingFace Datasets library (Lhoest et al., 2021). For both SGD and MultiWOZ we set the shared target values T as the [NONE] and [DONTCARE] tokens and include a special intent with the name \"NONE\" for each service which is used as the target intent when no other intent is active.", "publication": {"title": "KILT: a benchmark for knowledge intensive language tasks", "authors": ["Petroni, Fabio", "Piktus, Aleksandra", "Fan, Angela", "Lewis, Patrick", "Yazdani, Majid", "Cao, Nicola", "Thorne, James", "Jernite, Yacine", "Karpukhin, Vladimir", "Maillard, Jean", "Plachouras, Vassilis", "Rockt\u00e4schel, Tim", "Riedel, Sebastian"], "pub_date": "2021"}}, {"citation_text": "Specifically we use the KILT Wikipedia snapshot 1 from 2019 (Petroni et al., 2021) as provided by the HuggingFace Datasets library (Lhoest et al., 2021). For both SGD and MultiWOZ we set the shared target values T as the [NONE] and [DONTCARE] tokens and include a special intent with the name \"NONE\" for each service which is used as the target intent when no other intent is active.", "publication": {"title": "Datasets: A community library for natural language processing", "authors": ["Lhoest, Quentin", "Villanova Del Moral, Albert", "Jernite, Yacine", "Thakur, Abhishek", "Patrick Von Platen, Suraj", "Patil, Julien", "Chaumond, Mariama", "Drame, Julien", "Plu, Lewis", "Tunstall, Joe", "Davison, Mario", "\u0160a\u0161ko, Gunjan", "Chhablani, Bhavitvya", "Malik, Simon", "Brandeis, ", "Teven, Le", "Scao, Victor", "Sanh, Canwen", "Xu, Nicolas", "Patry, Angelina", "Mcmillan-Major, Philipp", "Schmid, Sylvain", "Gugger, Cl\u00e9ment", "Delangue, Th\u00e9o", "Matussi\u00e8re, Lysandre", "Debut, Stas", "Bekman, Pierric", "Cistac, Thibault", "Goehringer, Victor", "Mustar, Fran\u00e7ois", "Lagunas, Alexander", "Rush, Thomas", "Wolf, "], "pub_date": "2021"}}, {"citation_text": "SGD baseline (Rastogi et al., 2020) is a simple extractive BERT-based model which encodes the schema and last utterance separately and uses the embeddings in downstream classifiers to predict relative slot updates for the current turn. SGP-DST (Ruan et al., 2020) and DS-DST (Zhang et al., 2020) are similar but jointly encode utterance and slot schema.", "publication": {"title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset", "authors": ["Rastogi, Abhinav", "Zang, Xiaoxue", "Sunkara, Srinivas", "Gupta, Raghav", "Khaitan, Pranav"], "pub_date": "2020"}}, {"citation_text": "SGD baseline (Rastogi et al., 2020) is a simple extractive BERT-based model which encodes the schema and last utterance separately and uses the embeddings in downstream classifiers to predict relative slot updates for the current turn. SGP-DST (Ruan et al., 2020) and DS-DST (Zhang et al., 2020) are similar but jointly encode utterance and slot schema.", "publication": {"title": "Fine-tuning bert for schema-guided zero-shot dialogue state tracking", "authors": ["Ruan, Yu-Ping", "Ling, Zhen-Hua", "Gu, Jia-Chen", "Liu, Quan"], "pub_date": "2020"}}, {"citation_text": "SGD baseline (Rastogi et al., 2020) is a simple extractive BERT-based model which encodes the schema and last utterance separately and uses the embeddings in downstream classifiers to predict relative slot updates for the current turn. SGP-DST (Ruan et al., 2020) and DS-DST (Zhang et al., 2020) are similar but jointly encode utterance and slot schema.", "publication": {"title": "Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking", "authors": ["Zhang, Jianguo", "Hashimoto, Kazuma", "Wu, Chien-Sheng", "Wang, Yao", "Yu, Philip", "Socher, Richard", "Xiong, Caiming"], "pub_date": "2020"}}, {"citation_text": "SGP-DST (Ruan et al., 2020) and DS-DST (Zhang et al., 2020) are similar but jointly encode utterance and slot schema. Multi-Task BERT (Kapelonis et al., 2022) is also similar but uses system action annotations which include annotations of slots offered or requested by the system (e.g.", "publication": {"title": "Fine-tuning bert for schema-guided zero-shot dialogue state tracking", "authors": ["Ruan, Yu-Ping", "Ling, Zhen-Hua", "Gu, Jia-Chen", "Liu, Quan"], "pub_date": "2020"}}, {"citation_text": "SGP-DST (Ruan et al., 2020) and DS-DST (Zhang et al., 2020) are similar but jointly encode utterance and slot schema. Multi-Task BERT (Kapelonis et al., 2022) is also similar but uses system action annotations which include annotations of slots offered or requested by the system (e.g.", "publication": {"title": "Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking", "authors": ["Zhang, Jianguo", "Hashimoto, Kazuma", "Wu, Chien-Sheng", "Wang, Yao", "Yu, Philip", "Socher, Richard", "Xiong, Caiming"], "pub_date": "2020"}}, {"citation_text": "SGP-DST (Ruan et al., 2020) and DS-DST (Zhang et al., 2020) are similar but jointly encode utterance and slot schema. Multi-Task BERT (Kapelonis et al., 2022) is also similar but uses system action annotations which include annotations of slots offered or requested by the system (e.g.", "publication": {"title": "A multi-task bert model for schema-guided dialogue state tracking", "authors": ["Kapelonis, Eleftherios", "Georgiou, Efthymios", "Potamianos, Alexandros"], "pub_date": "2022"}}, {"citation_text": "Multi-Task BERT (Kapelonis et al., 2022) is also similar but uses system action annotations which include annotations of slots offered or requested by the system (e.g. Fremont\").", "publication": {"title": "A multi-task bert model for schema-guided dialogue state tracking", "authors": ["Kapelonis, Eleftherios", "Georgiou, Efthymios", "Potamianos, Alexandros"], "pub_date": "2022"}}, {"citation_text": "paDST (Ma et al., 2019) combines an extractive component for non-categorical slots with a classifier that uses 83 hand-crafted features (including system action annotations) for categorical slots. Additionally it augments training data via back-translation achieving strong results but making a direct comparison difficult.", "publication": {"title": "An end-to-end dialogue state tracking system with machine reading comprehension and wide & deep classification", "authors": ["Ma, Yue", "Zeng, Zengfeng", "Zhu, Dawei", "Li, Xuan", "Yang, Yiying", "Yao, Xiaoyuan", "Zhou, Kaijie", "Shen, Jianping"], "pub_date": "2019"}}, {"citation_text": "LUNA (Wang et al., 2022) separately encodes dialogue history, slots and slot values and learns to first predict the correct utterance to condition the slot value prediction on. Generative baselines.", "publication": {"title": "LUNA: Learning slot-turn alignment for dialogue state tracking", "authors": ["Wang, Yifan", "Zhao, Jing", "Bao, Junwei", "Duan, Chaoqun"], "pub_date": "2022"}}, {"citation_text": "Seq2Seq-DU (Feng et al., 2021) first separately encodes utterance and schema and then conditions the decoder on the cross-attended utterance and schema embeddings. The decoder generates a state representation consisting of pointers to schema elements and utterance tokens.", "publication": {"title": "A sequenceto-sequence approach to dialogue state tracking", "authors": ["Feng, Yue", "Wang, Yang", "Li, Hang"], "pub_date": "2021"}}, {"citation_text": "AG-DST (Tian et al., 2021) (Zang et al., 2020). \u2021: AG-DST uses a fixed two-pass generation procedure.", "publication": {"title": "MultiWOZ 2.2 : A dialogue dataset with additional annotation corrections and state tracking baselines", "authors": ["Zang, Xiaoxue", "Rastogi, Abhinav", "Sunkara, Srinivas", "Gupta, Raghav", "Zhang, Jianguo", "Chen, Jindong"], "pub_date": "2020"}}, {"citation_text": "AG-DST (Tian et al., 2021) (Zang et al., 2020). \u2021: AG-DST uses a fixed two-pass generation procedure.", "publication": {"title": "Amendable generation for dialogue state tracking", "authors": ["Tian, Xin", "Huang, Liankai", "Lin, Yingzhan", "Bao, Siqi", "He, Huang", "Yang, Yunyi", "Wu, Hua", "Wang, Fan", "Sun, Shuqi"], "pub_date": "2021"}}, {"citation_text": "DaP (Lee et al., 2021) comes in two variants which we denote as DaP (seq) and DaP (ind). DaP (ind) takes as input the entire dialogue history and an individual slot description and decodes the inferred slot value directly but requires one inference pass for each slot in the schema.", "publication": {"title": "Dialogue state tracking with a language model using schema-driven prompting", "authors": ["Lee, Chia-Hsuan", "Cheng, Hao", "Ostendorf, Mari"], "pub_date": "2021"}}, {"citation_text": "DaP (Lee et al., 2021) comes in two variants which we denote as DaP (seq) and DaP (ind). DaP (ind) takes as input the entire dialogue history and an individual slot description and decodes the inferred slot value directly but requires one inference pass for each slot in the schema.", "publication": {"title": "Dialogue state tracking with a language model using schema-driven prompting", "authors": ["Lee, Chia-Hsuan", "Cheng, Hao", "Ostendorf, Mari"], "pub_date": "2021"}}, {"citation_text": "D3ST (Zhao et al., 2022) takes a similar approach and decodes the entire dialogue state including the active intent in a single pass. Categorical slot values are predicted via an index-picking mechanism.", "publication": {"title": "Descriptiondriven task-oriented dialog modeling", "authors": ["Zhao, Jeffrey", "Gupta, Raghav", "Cao, Yuan", "Yu, Dian", "Wang, Mingqiu", "Lee, Harrison", "Rastogi, Abhinav", "Shafran, Izhak", "Wu, Yonghui"], "pub_date": "2022"}}, {"citation_text": "D3ST (Zhao et al., 2022) takes a similar approach and decodes the entire dialogue state including the active intent in a single pass. Categorical slot values are predicted via an index-picking mechanism.", "publication": {"title": "Descriptiondriven task-oriented dialog modeling", "authors": ["Zhao, Jeffrey", "Gupta, Raghav", "Cao, Yuan", "Yu, Dian", "Wang, Mingqiu", "Lee, Harrison", "Rastogi, Abhinav", "Shafran, Izhak", "Wu, Yonghui"], "pub_date": "2022"}}, {"citation_text": "We note that although paDST achieves the best performance of all baseline models in terms of JGA, it is not directly comparable because it is trained with hand-crafted features and additional back-translation data for training which has been shown to significantly improve robustness and generalization to unseen descriptions in schema-guided DST (Lee et al., 2022). Similarly, although Multi-Task BERT achieves good performance this can mostly be attributed to the use of system action annotation as Kapelonis et al.", "publication": {"title": "Sgd-x: A benchmark for robust generalization in schemaguided dialogue systems", "authors": ["Lee, Harrison", "Gupta, Raghav", "Rastogi, Abhinav", "Cao, Yuan", "Zhang, Bin", "Wu, Yonghui"], "pub_date": "2022"}}, {"citation_text": "Similarly, although Multi-Task BERT achieves good performance this can mostly be attributed to the use of system action annotation as Kapelonis et al. (2022) themselves demonstrate.", "publication": {"title": "A multi-task bert model for schema-guided dialogue state tracking", "authors": ["Kapelonis, Eleftherios", "Georgiou, Efthymios", "Potamianos, Alexandros"], "pub_date": "2022"}}, {"citation_text": "Schema-guided approaches address this by explicitly conditioning predictions on a variable schema which describes intents and slots in natural language (Rastogi et al., 2020). Both Ruan et al.", "publication": {"title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset", "authors": ["Rastogi, Abhinav", "Zang, Xiaoxue", "Sunkara, Srinivas", "Gupta, Raghav", "Khaitan, Pranav"], "pub_date": "2020"}}, {"citation_text": "Both Ruan et al. (2020) and Zhang et al.", "publication": {"title": "Fine-tuning bert for schema-guided zero-shot dialogue state tracking", "authors": ["Ruan, Yu-Ping", "Ling, Zhen-Hua", "Gu, Jia-Chen", "Liu, Quan"], "pub_date": "2020"}}, {"citation_text": "(2020) and Zhang et al. (2021) introduce schema-guided models but predict slots independently from one another requiring multiple encoder passes for each turn and failing to model intent-slot and inter-slot dependencies.", "publication": {"title": "Sgd-qa: Fast schema-guided dialogue state tracking for unseen services", "authors": ["Zhang, Yang", "Noroozi, Vahid", "Bakhturina, Evelina", "Ginsburg, Boris"], "pub_date": "2021"}}, {"citation_text": "Ma et al. (2019) use MRC for non-categorical and handcrafted features for categorical slots.", "publication": {"title": "An end-to-end dialogue state tracking system with machine reading comprehension and wide & deep classification", "authors": ["Ma, Yue", "Zeng, Zengfeng", "Zhu, Dawei", "Li, Xuan", "Yang, Yiying", "Yao, Xiaoyuan", "Zhou, Kaijie", "Shen, Jianping"], "pub_date": "2019"}}, {"citation_text": "In an attempt to address the lack of ability to generalize to new domains and ontologies, Wu et al. (2019) propose incorporating a generative component into DST.", "publication": {"title": "Transferable multi-domain state generator for task-oriented dialogue systems", "authors": ["Wu, Chien-Sheng", "Madotto, Andrea", "Hosseini-Asl, Ehsan", "Xiong, Caiming", "Socher, Richard", "Fung, Pascale"], "pub_date": "2019"}}, {"citation_text": "Feng et al. (2021) instead generate the entire state as a single sequence of pointers to the dialogue history and input schema but separately encode history and schema.", "publication": {"title": "A sequenceto-sequence approach to dialogue state tracking", "authors": ["Feng, Yue", "Wang, Yang", "Li, Hang"], "pub_date": "2021"}}, {"citation_text": "Zhao et al. (2021) model DST fully as a text-to-text problem and directly generate the entire current state as a string.", "publication": {"title": "Effective sequence-tosequence dialogue state tracking", "authors": ["Zhao, Jeffrey", "Mahdieh, Mahdis", "Zhang, Ye", "Cao, Yuan", "Wu, Yonghui"], "pub_date": "2021"}}, {"citation_text": "Lin et al. (2021) transfer a language model fine-tuned for seq2seq question answering to DST zero-shot using the dialog history as context and simply asking the model for the slot values.", "publication": {"title": "Amendable generation for dialogue state tracking", "authors": ["Tian, Xin", "Huang, Liankai", "Lin, Yingzhan", "Bao, Siqi", "He, Huang", "Yang, Yunyi", "Wu, Hua", "Wang, Fan", "Sun, Shuqi"], "pub_date": "2021"}}, {"citation_text": "By also including a natural language schema in the input, Zhao et al. (2022) show that full joint modeling and rich attention between history and schema lead to better results in DST.", "publication": {"title": "Descriptiondriven task-oriented dialog modeling", "authors": ["Zhao, Jeffrey", "Gupta, Raghav", "Cao, Yuan", "Yu, Dian", "Wang, Mingqiu", "Lee, Harrison", "Rastogi, Abhinav", "Shafran, Izhak", "Wu, Yonghui"], "pub_date": "2022"}}, {"citation_text": "Gupta et al. (2022) show the effectiveness of using demonstrations of slots being used in practice instead of a natural language descriptions in the prompt.", "publication": {"title": "Show, Don\u2019t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue", "authors": ["Gupta, Raghav", "Lee, Harrison", "Zhao, Jeffrey", "Cao, Yuan", "Rastogi, Abhinav", "Wu, Yonghui"], "pub_date": "2022"}}]}